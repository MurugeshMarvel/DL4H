{
 "metadata": {
  "name": "",
  "signature": "sha256:9235e6690ae045000a3d009e7ebd7ec4f2cfdadb87fbdb760d9c2cd39ff1cee1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\"\n",
      "A deep neural network with or w/o dropout in one notebook.\n",
      "\n",
      "To train the networks you just need to call the run() function\n",
      "You need to run each cell before doing so.\n",
      "To do so, you can go to \"Cell -> Run All\"\n",
      "Feel free to edit individually each cell and rerun modified ones.\n",
      "\n",
      "You can change global parameters here before running\n",
      "\"\"\"\n",
      "\n",
      "# Which experiment to run\n",
      "MNIST = True\n",
      "DIGITS = False\n",
      "FACES = False\n",
      "TWENTYNEWSGROUPS = False\n",
      "\n",
      "SCALE = True      # preprocessing scale \n",
      "BATCH_SIZE = 100  # default batch size\n",
      "L2_LAMBDA = 1.    # default L2 regularization parameter\n",
      "INIT_LR = 0.01    # initial learning rate, try making it larger"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import sys\n",
      "import math\n",
      "from theano import tensor as T\n",
      "from theano import shared\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from collections import OrderedDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Activation and helper functions\n",
      "\n",
      "def relu_f(vec):\n",
      "    \"\"\" Wrapper to quickly change the rectified linear unit function \"\"\"\n",
      "    return (vec + abs(vec)) / 2.\n",
      "\n",
      "\n",
      "def softplus_f(v):\n",
      "    return T.log(1 + T.exp(v))\n",
      "\n",
      "\n",
      "def dropout(rng, x, p=0.5):\n",
      "    \"\"\" Zero-out random values in x with probability p using rng \"\"\"\n",
      "    if p > 0. and p < 1.:\n",
      "        seed = rng.randint(2 ** 30)\n",
      "        srng = theano.tensor.shared_randomstreams.RandomStreams(seed)\n",
      "        mask = srng.binomial(n=1, p=1.-p, size=x.shape,\n",
      "                dtype=theano.config.floatX)\n",
      "        return x * mask\n",
      "    return x\n",
      "\n",
      "\n",
      "def build_shared_zeros(shape, name):\n",
      "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
      "    return shared(value=numpy.zeros(shape, dtype=theano.config.floatX),\n",
      "            name=name, borrow=True)\n",
      "\n",
      "\n",
      "class Linear(object):\n",
      "    \"\"\" Basic linear transformation layer (W.X + b) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(rng.uniform(\n",
      "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "            W_values *= 4  # This works for sigmoid activated networks!\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "        if b is None:\n",
      "            b = build_shared_zeros((n_out,), 'b')\n",
      "        self.input = input\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "        self.params = [self.W, self.b]\n",
      "        self.output = T.dot(self.input, self.W) + self.b\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"Linear\"\n",
      "\n",
      "\n",
      "class SigmoidLayer(Linear):\n",
      "    \"\"\" Sigmoid activation layer (sigmoid(W.X + b)) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        super(SigmoidLayer, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = T.nnet.sigmoid(self.pre_activation)\n",
      "\n",
      "\n",
      "class ReLU(Linear):\n",
      "    \"\"\" Rectified Linear Unit activation layer (max(0, W.X + b)) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if b is None:\n",
      "            b = build_shared_zeros((n_out,), 'b')\n",
      "        super(ReLU, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = relu_f(self.pre_activation)\n",
      "\n",
      "\n",
      "class SoftPlus(Linear):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "        super(SoftPlus, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = softplus_f(self.pre_activation)\n",
      "\n",
      "\n",
      "class DatasetMiniBatchIterator(object):\n",
      "    \"\"\" Basic mini-batch iterator \"\"\"\n",
      "    def __init__(self, x, y, batch_size=BATCH_SIZE, randomize=False):\n",
      "        self.x = x\n",
      "        self.y = y\n",
      "        self.batch_size = batch_size\n",
      "        self.randomize = randomize\n",
      "        from sklearn.utils import check_random_state\n",
      "        self.rng = check_random_state(42)\n",
      "\n",
      "    def __iter__(self):\n",
      "        n_samples = self.x.shape[0]\n",
      "        if self.randomize:\n",
      "            for _ in xrange(n_samples / BATCH_SIZE):\n",
      "                if BATCH_SIZE > 1:\n",
      "                    i = int(self.rng.rand(1) * ((n_samples+BATCH_SIZE-1) / BATCH_SIZE))\n",
      "                else:\n",
      "                    i = int(math.floor(self.rng.rand(1) * n_samples))\n",
      "                yield (i, self.x[i*self.batch_size:(i+1)*self.batch_size],\n",
      "                       self.y[i*self.batch_size:(i+1)*self.batch_size])\n",
      "        else:\n",
      "            for i in xrange((n_samples + self.batch_size - 1)\n",
      "                            / self.batch_size):\n",
      "                yield (self.x[i*self.batch_size:(i+1)*self.batch_size],\n",
      "                       self.y[i*self.batch_size:(i+1)*self.batch_size])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression:\n",
      "    \"\"\" _Multi-class_ Logistic Regression \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if W != None:\n",
      "            self.W = W\n",
      "        else:\n",
      "            self.W = build_shared_zeros((n_in, n_out), 'W')\n",
      "        if b != None:\n",
      "            self.b = b\n",
      "        else:\n",
      "            self.b = build_shared_zeros((n_out,), 'b')\n",
      "        self.input = input\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        self.output = self.y_pred\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def negative_log_likelihood_sum(self, y):\n",
      "        return -T.sum(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def training_cost(self, y):\n",
      "        \"\"\" Wrapper for standard name \"\"\"\n",
      "        return self.negative_log_likelihood(y)\n",
      "\n",
      "    def errors(self, y):\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\"!!! 'y' should have the same shape as 'self.y_pred'\",\n",
      "                (\"y\", y.type, \"y_pred\", self.y_pred.type))\n",
      "        if y.dtype.startswith('int'):\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            print(\"!!! y should be of int type\")\n",
      "            return T.mean(T.neq(self.y_pred, numpy.asarray(y, dtype='int')))\n",
      "        \n",
      "        \n",
      "#class PerceptronLoss: # TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNet(object):\n",
      "    \"\"\" Neural network (not regularized, without dropout) \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None, \n",
      "                 n_ins=40*3,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[1024, 1024, 1024, 1024],\n",
      "                 n_outs=62*3,\n",
      "                 rho=0.95, eps=1.E-6,\n",
      "                 momentum=0.9, step_adapt_alpha=1.E-4,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        Basic Neural Net class\n",
      "        \"\"\"\n",
      "        self.layers = []\n",
      "        self.params = []\n",
      "        self.n_layers = len(layers_types)\n",
      "        self.layers_types = layers_types\n",
      "        assert self.n_layers > 0\n",
      "        self._rho = rho  # ``momentum'' for adadelta (and discount/decay for RMSprop)\n",
      "        self._eps = eps  # epsilon for adadelta (and for RMSprop)\n",
      "        self._momentum = momentum  # for RMSProp\n",
      "        self._accugrads = []  # for adadelta\n",
      "        self._accudeltas = []  # for adadelta\n",
      "        self._avggrads = []  # for RMSprop in the Alex Graves' variant\n",
      "        self._stepadapts = []  # for RMSprop with step adaptations\n",
      "        self._stepadapt_alpha = step_adapt_alpha\n",
      "\n",
      "        if theano_rng == None:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        self.x = T.fmatrix('x')\n",
      "        self.y = T.ivector('y')\n",
      "        \n",
      "        self.layers_ins = [n_ins] + layers_sizes\n",
      "        self.layers_outs = layers_sizes + [n_outs]\n",
      "        \n",
      "        layer_input = self.x\n",
      "        \n",
      "        for layer_type, n_in, n_out in zip(layers_types,\n",
      "                self.layers_ins, self.layers_outs):\n",
      "            this_layer = layer_type(rng=numpy_rng,\n",
      "                    input=layer_input, n_in=n_in, n_out=n_out)\n",
      "            assert hasattr(this_layer, 'output')\n",
      "            self.params.extend(this_layer.params)\n",
      "            self._accugrads.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'accugrad') for t in this_layer.params])\n",
      "            self._accudeltas.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'accudelta') for t in this_layer.params])\n",
      "            self._avggrads.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'avggrad') for t in this_layer.params])\n",
      "            self._stepadapts.extend([shared(value=numpy.ones(t.shape.eval(),\n",
      "                dtype=theano.config.floatX),\n",
      "                name='stepadapt', borrow=True) for t in this_layer.params])\n",
      "            self.layers.append(this_layer)\n",
      "            layer_input = this_layer.output\n",
      "\n",
      "        assert hasattr(self.layers[-1], 'training_cost')\n",
      "        assert hasattr(self.layers[-1], 'errors')\n",
      "        self.mean_cost = self.layers[-1].negative_log_likelihood(self.y)\n",
      "        self.cost = self.layers[-1].training_cost(self.y)\n",
      "        if debugprint:\n",
      "            theano.printing.debugprint(self.cost)\n",
      "\n",
      "        self.errors = self.layers[-1].errors(self.y)\n",
      "\n",
      "    def __repr__(self):\n",
      "        dimensions_layers_str = map(lambda x: \"x\".join(map(str, x)),\n",
      "                                    zip(self.layers_ins, self.layers_outs))\n",
      "        return \"_\".join(map(lambda x: \"_\".join((x[0].__name__, x[1])),\n",
      "                            zip(self.layers_types, dimensions_layers_str)))\n",
      "\n",
      "\n",
      "    def get_SGD_trainer(self):\n",
      "        \"\"\" Returns a plain SGD minibatch trainer with learning rate as param. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)  # all the gradients\n",
      "        updates = OrderedDict()\n",
      "        for param, gparam in zip(self.params, gparams):\n",
      "            updates[param] = param - gparam * learning_rate\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y),\n",
      "                                           theano.Param(learning_rate)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_adagrad_trainer(self):\n",
      "        \"\"\" Returns an Adagrad (Duchi et al. 2010) trainer using a learning rate.\n",
      "        \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)  # all the gradients\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, param, gparam in zip(self._accugrads, self.params, gparams):\n",
      "            # c.f. Algorithm 1 in the Adadelta paper (Zeiler 2012)\n",
      "            agrad = accugrad + gparam * gparam\n",
      "            dx = - (learning_rate / T.sqrt(agrad + self._eps)) * gparam\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = agrad\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x), \n",
      "            theano.Param(batch_y),\n",
      "            theano.Param(learning_rate)],\n",
      "            outputs=self.mean_cost,\n",
      "            updates=updates,\n",
      "            givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_adadelta_trainer(self):\n",
      "        \"\"\" Returns an Adadelta (Zeiler 2012) trainer using self._rho and\n",
      "        self._eps params. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        gparams = T.grad(self.mean_cost, self.params)\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, accudelta, param, gparam in zip(self._accugrads,\n",
      "                self._accudeltas, self.params, gparams):\n",
      "            # c.f. Algorithm 1 in the Adadelta paper (Zeiler 2012)\n",
      "            agrad = self._rho * accugrad + (1 - self._rho) * gparam * gparam\n",
      "            dx = - T.sqrt((accudelta + self._eps)\n",
      "                          / (agrad + self._eps)) * gparam\n",
      "            updates[accudelta] = (self._rho * accudelta\n",
      "                                  + (1 - self._rho) * dx * dx)\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = agrad\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_rmsprop_trainer(self, with_step_adapt=True, nesterov=False):  # TODO Nesterov momentum\n",
      "        \"\"\" Returns an RmsProp (possibly Nesterov) (Sutskever 2013) trainer\n",
      "        using self._rho, self._eps and self._momentum params. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, avggrad, accudelta, sa, param, gparam in zip(\n",
      "                self._accugrads, self._avggrads, self._accudeltas,\n",
      "                self._stepadapts, self.params, gparams):\n",
      "            acc_grad = self._rho * accugrad + (1 - self._rho) * gparam * gparam\n",
      "            avg_grad = self._rho * avggrad + (1 - self._rho) * gparam  # this decay/discount (self._rho) should differ from the one of the line above\n",
      "            ###scaled_grad = gparam / T.sqrt(acc_grad + self._eps)  # original RMSprop gradient scaling\n",
      "            scaled_grad = gparam / T.sqrt(acc_grad - avg_grad**2 + self._eps)  # Alex Graves' RMSprop variant (divide by a \"running stddev\" of the updates)\n",
      "            if with_step_adapt:\n",
      "                incr = sa * (1. + self._stepadapt_alpha)\n",
      "                #decr = sa * (1. - self._stepadapt_alpha)\n",
      "                decr = sa * (1. - 2*self._stepadapt_alpha)\n",
      "                ###steps = sa * T.switch(accudelta * -gparam >= 0, incr, decr)\n",
      "                steps = T.clip(T.switch(accudelta * -gparam >= 0, incr, decr), self._eps, 1./self._eps)  # bad overloading of self._eps!\n",
      "                scaled_grad = steps * scaled_grad\n",
      "                updates[sa] = steps\n",
      "            dx = self._momentum * accudelta - learning_rate * scaled_grad\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = acc_grad\n",
      "            updates[avggrad] = avg_grad\n",
      "            updates[accudelta] = dx\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y),\n",
      "                                           theano.Param(learning_rate)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def score_classif(self, given_set):\n",
      "        \"\"\" Returns functions to get current classification errors. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        score = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                        theano.Param(batch_y)],\n",
      "                                outputs=self.errors,\n",
      "                                givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        def scoref():\n",
      "            \"\"\" returned function that scans the entire set given as input \"\"\"\n",
      "            return [score(batch_x, batch_y) for batch_x, batch_y in given_set]\n",
      "\n",
      "        return scoref\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RegularizedNet(NeuralNet):\n",
      "    \"\"\" Neural net with L1 and L2 regularization \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None,\n",
      "                 n_ins=100,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[1024, 1024, 1024],\n",
      "                 n_outs=2,\n",
      "                 rho=0.95, eps=1.E-6,\n",
      "                 L1_reg=0.1,\n",
      "                 L2_reg=0.1,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        A deep neural net with possible L1 and/or L2 regularization.\n",
      "        \"\"\"\n",
      "        super(RegularizedNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
      "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
      "\n",
      "        self.L1_reg = L1_reg\n",
      "        self.L2_reg = L2_reg\n",
      "        L1 = shared(0.)\n",
      "        for param in self.params:\n",
      "            L1 += T.sum(abs(param))\n",
      "        if L1_reg > 0.:\n",
      "            self.cost = self.cost + L1_reg * L1\n",
      "        L2 = shared(0.)\n",
      "        for param in self.params:\n",
      "            L2 += T.sum(param ** 2)\n",
      "        if L2_reg > 0.:\n",
      "            self.cost = self.cost + L2_reg * L2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DropoutNet(NeuralNet):\n",
      "    \"\"\" Neural net with dropout (see Hinton's et al. paper) \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None,\n",
      "                 n_ins=40*3,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[4000, 4000, 4000, 4000],\n",
      "                 dropout_rates=[0.2, 0.5, 0.5, 0.5, 0.5],\n",
      "                 n_outs=62 * 3,\n",
      "                 rho=0.98, eps=1.E-6,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        A dropout-regularized neural net.\n",
      "        \"\"\"\n",
      "        super(DropoutNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
      "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
      "\n",
      "        self.dropout_rates = dropout_rates\n",
      "        dropout_layer_input = dropout(numpy_rng, self.x, p=dropout_rates[0])\n",
      "        self.dropout_layers = []\n",
      "\n",
      "        for layer, layer_type, n_in, n_out, dr in zip(self.layers,\n",
      "                layers_types, self.layers_ins, self.layers_outs,\n",
      "                dropout_rates[1:] + [0]):  # !!! we do not dropout anything\n",
      "                                           # from the last layer !!!\n",
      "            if dr:\n",
      "                this_layer = layer_type(rng=numpy_rng,\n",
      "                        input=dropout_layer_input, n_in=n_in, n_out=n_out,\n",
      "                        W=layer.W * 1. / (1. - dr),\n",
      "                        b=layer.b * 1. / (1. - dr))\n",
      "                # N.B. dropout with dr==1 does not dropanything!!\n",
      "                this_layer.output = dropout(numpy_rng, this_layer.output, dr)\n",
      "            else:\n",
      "                this_layer = layer_type(rng=numpy_rng,\n",
      "                        input=dropout_layer_input, n_in=n_in, n_out=n_out,\n",
      "                        W=layer.W, b=layer.b)\n",
      "\n",
      "            assert hasattr(this_layer, 'output')\n",
      "            self.dropout_layers.append(this_layer)\n",
      "            dropout_layer_input = this_layer.output\n",
      "\n",
      "        assert hasattr(self.layers[-1], 'training_cost')\n",
      "        assert hasattr(self.layers[-1], 'errors')\n",
      "        # TODO standardize cost\n",
      "        # these are the dropout costs\n",
      "        self.mean_cost = self.dropout_layers[-1].negative_log_likelihood(self.y)\n",
      "        self.cost = self.dropout_layers[-1].training_cost(self.y)\n",
      "\n",
      "        # these is the non-dropout errors\n",
      "        self.errors = self.layers[-1].errors(self.y)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return super(DropoutNet, self).__repr__() + \"\\n\"\\\n",
      "                + \"dropout rates: \" + str(self.dropout_rates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_fit_and_score(class_to_chg):\n",
      "    \"\"\" Mutates a class to add the fit() and score() functions to a NeuralNet.\n",
      "    \"\"\"\n",
      "    from types import MethodType\n",
      "    def fit(self, x_train, y_train, x_dev=None, y_dev=None,\n",
      "            max_epochs=20, early_stopping=True, split_ratio=0.1, # TODO 100+ epochs\n",
      "            method='adadelta', verbose=False, plot=False):\n",
      "        \"\"\"\n",
      "        TODO\n",
      "        \"\"\"\n",
      "        import time, copy\n",
      "        if x_dev == None or y_dev == None:\n",
      "            from sklearn.cross_validation import train_test_split\n",
      "            x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train,\n",
      "                    test_size=split_ratio, random_state=42)\n",
      "        if method == 'sgd':\n",
      "            train_fn = self.get_SGD_trainer()\n",
      "        elif method == 'adagrad':\n",
      "            train_fn = self.get_adagrad_trainer()\n",
      "        elif method == 'adadelta':\n",
      "            train_fn = self.get_adadelta_trainer()\n",
      "        elif method == 'rmsprop':\n",
      "            train_fn = self.get_rmsprop_trainer(with_step_adapt=True,\n",
      "                    nesterov=False)\n",
      "        train_set_iterator = DatasetMiniBatchIterator(x_train, y_train)\n",
      "        dev_set_iterator = DatasetMiniBatchIterator(x_dev, y_dev)\n",
      "        train_scoref = self.score_classif(train_set_iterator)\n",
      "        dev_scoref = self.score_classif(dev_set_iterator)\n",
      "        best_dev_loss = numpy.inf\n",
      "        epoch = 0\n",
      "        # TODO early stopping (not just cross val, also stop training)\n",
      "        if plot:\n",
      "            verbose = True\n",
      "            self._costs = []\n",
      "            self._train_errors = []\n",
      "            self._dev_errors = []\n",
      "            self._updates = []\n",
      "\n",
      "        init_lr = INIT_LR\n",
      "        if method == 'rmsprop':\n",
      "            init_lr = 1.E-6  # TODO REMOVE HACK\n",
      "        n_seen = 0\n",
      "        while epoch < max_epochs:\n",
      "            #first TODO: update learning rates when not using adadelta!\n",
      "            #lr = init_lr / (1 + init_lr * L2_LAMBDA * math.log(1+n_seen))\n",
      "            #lr = init_lr / math.sqrt(1 + init_lr * L2_LAMBDA * n_seen/BATCH_SIZE) # try these\n",
      "            lr = init_lr\n",
      "            if not verbose:\n",
      "                sys.stdout.write(\"\\r%0.2f%%\" % (epoch * 100./ max_epochs))\n",
      "                sys.stdout.flush()\n",
      "            avg_costs = []\n",
      "            timer = time.time()\n",
      "            for x, y in train_set_iterator:\n",
      "                if method == 'sgd' or method == 'adagrad' or method == 'rmsprop':\n",
      "                    #avg_cost = train_fn(x, y, lr=1.E-2)\n",
      "                    avg_cost = train_fn(x, y, lr=lr)\n",
      "                elif method == 'adadelta':\n",
      "                    avg_cost = train_fn(x, y)\n",
      "                elif method == 'rmsprop':\n",
      "                    avg_cost = train_fn(x, y, lr=lr)\n",
      "                if type(avg_cost) == list:\n",
      "                    avg_costs.append(avg_cost[0])\n",
      "                else:\n",
      "                    avg_costs.append(avg_cost)\n",
      "            if verbose:\n",
      "                mean_costs = numpy.mean(avg_costs)\n",
      "                mean_train_errors = numpy.mean(train_scoref())\n",
      "                print('  epoch %i took %f seconds' %\n",
      "                      (epoch, time.time() - timer))\n",
      "                print('  epoch %i, avg costs %f' %\n",
      "                      (epoch, mean_costs))\n",
      "                print('  epoch %i, training error %f' %\n",
      "                      (epoch, mean_train_errors))\n",
      "                if plot:\n",
      "                    self._costs.append(mean_costs)\n",
      "                    self._train_errors.append(mean_train_errors)\n",
      "            dev_errors = numpy.mean(dev_scoref())\n",
      "            if plot:\n",
      "                self._dev_errors.append(dev_errors)\n",
      "            if dev_errors < best_dev_loss:\n",
      "                best_dev_loss = dev_errors\n",
      "                best_params = copy.deepcopy(self.params)\n",
      "                if verbose:\n",
      "                    print('!!!  epoch %i, validation error of best model %f' %\n",
      "                          (epoch, dev_errors))\n",
      "            epoch += 1\n",
      "            n_seen += x_train.shape[0]\n",
      "        if not verbose:\n",
      "            print(\"\")\n",
      "        for i, param in enumerate(best_params):\n",
      "            self.params[i] = param\n",
      "\n",
      "    def score(self, x, y):\n",
      "        \"\"\" error rates \"\"\"\n",
      "        iterator = DatasetMiniBatchIterator(x, y)\n",
      "        scoref = self.score_classif(iterator)\n",
      "        return numpy.mean(scoref())\n",
      "\n",
      "    class_to_chg.fit = MethodType(fit, None, class_to_chg)\n",
      "    class_to_chg.score = MethodType(score, None, class_to_chg)\n",
      "\n",
      "\n",
      "def train_models(x_train, y_train, x_test, y_test, n_features, n_outs,\n",
      "        x_dev=None, y_dev=None,\n",
      "        use_dropout=False, n_epochs=100, numpy_rng=None,\n",
      "        svms=False, nb=False, deepnn=True,\n",
      "        verbose=False, plot=False, name=''):\n",
      "    if svms:\n",
      "        print(\"Linear SVM\")\n",
      "        classifier = svm.SVC(gamma=0.001)\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "        print(\"RBF-kernel SVM\")\n",
      "        classifier = svm.SVC(kernel='rbf', class_weight='auto')\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "    if nb:\n",
      "        print(\"Multinomial Naive Bayes\")\n",
      "        classifier = naive_bayes.MultinomialNB()\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "    if deepnn:\n",
      "        import warnings\n",
      "        warnings.filterwarnings(\"ignore\")  # TODO remove\n",
      "\n",
      "        if use_dropout:\n",
      "            n_epochs *= 4\n",
      "            pass\n",
      "\n",
      "        def new_dnn(dropout=False):\n",
      "            if dropout:\n",
      "                print(\"Dropout DNN\")\n",
      "                return DropoutNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
      "                    layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                    layers_sizes=[2000, 2000, 2000, 2000],\n",
      "                    dropout_rates=[0.2, 0.5, 0.5, 0.5, 0.5],\n",
      "                    n_outs=n_outs,\n",
      "                    debugprint=0)\n",
      "            else:\n",
      "                print(\"Simple (regularized) DNN\")\n",
      "                return RegularizedNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
      "                    #layers_types=[LogisticRegression],\n",
      "                    #layers_sizes=[],\n",
      "                    #layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                    #layers_sizes=[1000, 1000, 1000],\n",
      "                    layers_types=[ReLU, LogisticRegression],\n",
      "                    layers_sizes=[200],\n",
      "                    n_outs=n_outs,\n",
      "                    L1_reg=0.,\n",
      "                    L2_reg=L2_LAMBDA,\n",
      "                    debugprint=1)\n",
      "\n",
      "        import matplotlib.pyplot as plt\n",
      "        plt.figure()\n",
      "        ax1 = plt.subplot(221)\n",
      "        ax2 = plt.subplot(222)\n",
      "        ax3 = plt.subplot(223)\n",
      "        ax4 = plt.subplot(224)  # TODO updates of the weights\n",
      "        #methods = ['sgd', 'adagrad', 'adadelta']\n",
      "        methods = ['adagrad', 'adadelta']\n",
      "        #methods = ['rmsprop', 'adadelta', 'adagrad']\n",
      "        for method in methods:\n",
      "            dnn = new_dnn(use_dropout)\n",
      "            print dnn\n",
      "            dnn.fit(x_train, y_train, x_dev, y_dev, max_epochs=n_epochs,\n",
      "                    method=method, verbose=verbose, plot=plot)\n",
      "            test_error = dnn.score(x_test, y_test)\n",
      "            print(\"score: %f\" % (1. - test_error))\n",
      "            ax1.plot(numpy.log10(dnn._costs), label=method)\n",
      "            #ax2.plot(numpy.log10(dnn._train_errors), label=method)\n",
      "            #ax3.plot(numpy.log10(dnn._dev_errors), label=method)\n",
      "            ax2.plot(dnn._train_errors, label=method)\n",
      "            ax3.plot(dnn._dev_errors, label=method)\n",
      "            #ax4.plot(dnn._updates, label=method) TODO\n",
      "            ax4.plot([test_error for _ in range(10)], label=method)\n",
      "        ax1.set_xlabel('epoch')\n",
      "        ax1.set_ylabel('cost (log10)')\n",
      "        ax2.set_xlabel('epoch')\n",
      "        ax2.set_ylabel('train error')\n",
      "        ax3.set_xlabel('epoch')\n",
      "        ax3.set_ylabel('dev error')\n",
      "        ax4.set_ylabel('test error')\n",
      "        plt.legend()\n",
      "        plt.tight_layout()\n",
      "        plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#main function\n",
      "\n",
      "def run():\n",
      "    add_fit_and_score(DropoutNet)\n",
      "    add_fit_and_score(RegularizedNet)\n",
      "\n",
      "    def nudge_dataset(X, Y):\n",
      "        \"\"\"\n",
      "        This produces a dataset 5 times bigger than the original one,\n",
      "        by moving the 8x8 images in X around by 1px to left, right, down, up\n",
      "        \"\"\"\n",
      "        from scipy.ndimage import convolve\n",
      "        direction_vectors = [\n",
      "            [[0, 1, 0],\n",
      "             [0, 0, 0],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [1, 0, 0],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [0, 0, 1],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [0, 0, 0],\n",
      "             [0, 1, 0]]]\n",
      "        shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
      "                                      weights=w).ravel()\n",
      "        X = numpy.concatenate([X] +\n",
      "                              [numpy.apply_along_axis(shift, 1, X, vector)\n",
      "                                  for vector in direction_vectors])\n",
      "        Y = numpy.concatenate([Y for _ in range(5)], axis=0)\n",
      "        return X, Y\n",
      "\n",
      "    from sklearn import datasets, svm, naive_bayes\n",
      "    from sklearn import cross_validation, preprocessing\n",
      "    \n",
      "\n",
      "    if MNIST:\n",
      "        from sklearn.datasets import fetch_mldata\n",
      "        mnist = fetch_mldata('MNIST original')\n",
      "        X = numpy.asarray(mnist.data, dtype='float32')\n",
      "        if SCALE:\n",
      "            #X = preprocessing.scale(X)\n",
      "            X /= 255.\n",
      "        y = numpy.asarray(mnist.target, dtype='int32')\n",
      "        print(\"Total dataset size:\")\n",
      "        print(\"n samples: %d\" % X.shape[0])\n",
      "        print(\"n features: %d\" % X.shape[1])\n",
      "        print(\"n classes: %d\" % len(set(y)))\n",
      "        x_train, x_test = X[:-10000], X[-10000:]\n",
      "        y_train, y_test = y[:-10000], y[-10000:]\n",
      "\n",
      "        train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "                     len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     use_dropout=False, n_epochs=20,\n",
      "                     verbose=True, plot=True, name='mnist_L2')\n",
      "        #train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "        #             len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "        #             use_dropout=True,\n",
      "        #             verbose=True, plot=True, name='mnist_dropout')\n",
      "\n",
      "    if DIGITS:\n",
      "        digits = datasets.load_digits()\n",
      "        data = numpy.asarray(digits.data, dtype='float32')\n",
      "        target = numpy.asarray(digits.target, dtype='int32')\n",
      "        nudged_x, nudged_y = nudge_dataset(data, target)\n",
      "        if SCALE:\n",
      "            nudged_x = preprocessing.scale(nudged_x)\n",
      "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
      "                nudged_x, nudged_y, test_size=0.2, random_state=42)\n",
      "        train_models(x_train, y_train, x_test, y_test, nudged_x.shape[1],\n",
      "                     len(set(target)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     verbose=True, plot=True, name='digits')\n",
      "\n",
      "    if FACES:\n",
      "        import logging\n",
      "        logging.basicConfig(level=logging.INFO,\n",
      "                            format='%(asctime)s %(message)s')\n",
      "        lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70,\n",
      "                                               resize=0.4)\n",
      "        X = numpy.asarray(lfw_people.data, dtype='float32')\n",
      "        if SCALE:\n",
      "            X = preprocessing.scale(X)\n",
      "        y = numpy.asarray(lfw_people.target, dtype='int32')\n",
      "        target_names = lfw_people.target_names\n",
      "        print(\"Total dataset size:\")\n",
      "        print(\"n samples: %d\" % X.shape[0])\n",
      "        print(\"n features: %d\" % X.shape[1])\n",
      "        print(\"n classes: %d\" % target_names.shape[0])\n",
      "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
      "                    X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "        train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "                     len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     verbose=True, plot=True, name='faces')\n",
      "\n",
      "    if TWENTYNEWSGROUPS:\n",
      "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "        newsgroups_train = datasets.fetch_20newsgroups(subset='train')\n",
      "        vectorizer = TfidfVectorizer(encoding='latin-1', max_features=10000)\n",
      "        #vectorizer = HashingVectorizer(encoding='latin-1')\n",
      "        x_train = vectorizer.fit_transform(newsgroups_train.data)\n",
      "        x_train = numpy.asarray(x_train.todense(), dtype='float32')\n",
      "        y_train = numpy.asarray(newsgroups_train.target, dtype='int32')\n",
      "        newsgroups_test = datasets.fetch_20newsgroups(subset='test')\n",
      "        x_test = vectorizer.transform(newsgroups_test.data)\n",
      "        x_test = numpy.asarray(x_test.todense(), dtype='float32')\n",
      "        y_test = numpy.asarray(newsgroups_test.target, dtype='int32')\n",
      "        train_models(x_train, y_train, x_test, y_test, x_train.shape[1],\n",
      "                     len(set(y_train)),\n",
      "                     numpy_rng=numpy.random.RandomState(123),\n",
      "                     svms=False, nb=True, deepnn=True,\n",
      "                     verbose=True, plot=True, name='20newsgroups')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total dataset size:\n",
        "n samples: 70000\n",
        "n features: 784\n",
        "n classes: 10\n",
        "Simple (regularized) DNN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ReLU_784x200_LogisticRegression_200x10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 0 took 1.400473 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 0, avg costs 0.319092\n",
        "  epoch 0, training error 0.059500\n",
        "!!!  epoch 0, validation error of best model 0.067167\n",
        "  epoch 1 took 1.411911 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 1, avg costs 0.192511\n",
        "  epoch 1, training error 0.047315\n",
        "!!!  epoch 1, validation error of best model 0.055167\n",
        "  epoch 2 took 1.432615 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 2, avg costs 0.159953\n",
        "  epoch 2, training error 0.040685\n",
        "!!!  epoch 2, validation error of best model 0.048667\n",
        "  epoch 3 took 1.427057 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 3, avg costs 0.140383\n",
        "  epoch 3, training error 0.036352\n",
        "!!!  epoch 3, validation error of best model 0.044667\n",
        "  epoch 4 took 1.420771 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 4, avg costs 0.126674\n",
        "  epoch 4, training error 0.033241\n",
        "!!!  epoch 4, validation error of best model 0.042667\n",
        "  epoch 5 took 1.404267 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 5, avg costs 0.116315\n",
        "  epoch 5, training error 0.030685\n",
        "!!!  epoch 5, validation error of best model 0.040500\n",
        "  epoch 6 took 1.414051 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 6, avg costs 0.108053\n",
        "  epoch 6, training error 0.028667\n",
        "!!!  epoch 6, validation error of best model 0.039500\n",
        "  epoch 7 took 1.426244 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 7, avg costs 0.101211\n",
        "  epoch 7, training error 0.026463\n",
        "!!!  epoch 7, validation error of best model 0.037500\n",
        "  epoch 8 took 1.420983 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 8, avg costs 0.095410\n",
        "  epoch 8, training error 0.024796\n",
        "!!!  epoch 8, validation error of best model 0.036500\n",
        "  epoch 9 took 1.403486 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 9, avg costs 0.090386\n",
        "  epoch 9, training error 0.023537\n",
        "!!!  epoch 9, validation error of best model 0.036167\n",
        "  epoch 10 took 1.431859 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 10, avg costs 0.085977\n",
        "  epoch 10, training error 0.022574\n",
        "!!!  epoch 10, validation error of best model 0.035500\n",
        "  epoch 11 took 1.430972 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 11, avg costs 0.082053\n",
        "  epoch 11, training error 0.021444\n",
        "!!!  epoch 11, validation error of best model 0.034333\n",
        "  epoch 12 took 1.395887 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 12, avg costs 0.078539\n",
        "  epoch 12, training error 0.020519\n",
        "!!!  epoch 12, validation error of best model 0.033833\n",
        "  epoch 13 took 1.438390 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 13, avg costs 0.075359\n",
        "  epoch 13, training error 0.019574\n",
        "!!!  epoch 13, validation error of best model 0.033500\n",
        "  epoch 14 took 1.419388 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 14, avg costs 0.072460\n",
        "  epoch 14, training error 0.018574\n",
        "!!!  epoch 14, validation error of best model 0.032833\n",
        "  epoch 15 took 1.403424 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 15, avg costs 0.069801\n",
        "  epoch 15, training error 0.017778\n",
        "!!!  epoch 15, validation error of best model 0.032500\n",
        "  epoch 16 took 1.438185 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 16, avg costs 0.067354\n",
        "  epoch 16, training error 0.017241\n",
        "!!!  epoch 16, validation error of best model 0.032167\n",
        "  epoch 17 took 1.423830 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 17, avg costs 0.065099\n",
        "  epoch 17, training error 0.016537\n",
        "!!!  epoch 17, validation error of best model 0.031833\n",
        "  epoch 18 took 1.390455 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 18, avg costs 0.062996\n",
        "  epoch 18, training error 0.015981\n",
        "!!!  epoch 18, validation error of best model 0.031500\n",
        "  epoch 19 took 1.474621 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 19, avg costs 0.061035\n",
        "  epoch 19, training error 0.015296\n",
        "score: 0.972600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Simple (regularized) DNN\n",
        "ReLU_784x200_LogisticRegression_200x10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 0 took 1.913752 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 0, avg costs 0.321439\n",
        "  epoch 0, training error 0.048130\n",
        "!!!  epoch 0, validation error of best model 0.055833\n",
        "  epoch 1 took 1.911449 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 1, avg costs 0.137223\n",
        "  epoch 1, training error 0.029500\n",
        "!!!  epoch 1, validation error of best model 0.041333\n",
        "  epoch 2 took 1.994624 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 2, avg costs 0.092100\n",
        "  epoch 2, training error 0.021759\n",
        "!!!  epoch 2, validation error of best model 0.037167\n",
        "  epoch 3 took 2.046144 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 3, avg costs 0.067929\n",
        "  epoch 3, training error 0.016796\n",
        "!!!  epoch 3, validation error of best model 0.034500\n",
        "  epoch 4 took 2.155358 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 4, avg costs 0.052133\n",
        "  epoch 4, training error 0.012630\n",
        "!!!  epoch 4, validation error of best model 0.032167"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 5 took 2.115944 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 5, avg costs 0.040443\n",
        "  epoch 5, training error 0.009889\n",
        "!!!  epoch 5, validation error of best model 0.029167\n",
        "  epoch 6 took 2.061319 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 6, avg costs 0.031359\n",
        "  epoch 6, training error 0.007019\n",
        "!!!  epoch 6, validation error of best model 0.027667\n",
        "  epoch 7 took 2.143759 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 7, avg costs 0.024055\n",
        "  epoch 7, training error 0.005204\n",
        "!!!  epoch 7, validation error of best model 0.026000\n",
        "  epoch 8 took 2.170452 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 8, avg costs 0.018230\n",
        "  epoch 8, training error 0.003685\n",
        "!!!  epoch 8, validation error of best model 0.024500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 9 took 2.249509 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 9, avg costs 0.013789\n",
        "  epoch 9, training error 0.002722\n",
        "!!!  epoch 9, validation error of best model 0.024000\n",
        "  epoch 10 took 2.081501 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 10, avg costs 0.010468\n",
        "  epoch 10, training error 0.002148\n",
        "!!!  epoch 10, validation error of best model 0.023500\n",
        "  epoch 11 took 2.245790 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 11, avg costs 0.008028\n",
        "  epoch 11, training error 0.001630\n",
        "!!!  epoch 11, validation error of best model 0.023500\n",
        "  epoch 12 took 2.146300 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 12, avg costs 0.006202\n",
        "  epoch 12, training error 0.001093\n",
        "  epoch 13 took 2.082400 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 13, avg costs 0.004864\n",
        "  epoch 13, training error 0.000704\n",
        "  epoch 14 took 2.048887 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 14, avg costs 0.003830\n",
        "  epoch 14, training error 0.000463\n",
        "  epoch 15 took 2.189157 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 15, avg costs 0.003062\n",
        "  epoch 15, training error 0.000352\n",
        "  epoch 16 took 2.140532 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 16, avg costs 0.002477\n",
        "  epoch 16, training error 0.000333\n",
        "  epoch 17 took 2.070620 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 17, avg costs 0.002071\n",
        "  epoch 17, training error 0.000278\n",
        "  epoch 18 took 2.071126 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 18, avg costs 0.001761\n",
        "  epoch 18, training error 0.000241\n",
        "  epoch 19 took 2.102216 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  epoch 19, avg costs 0.001517\n",
        "  epoch 19, training error 0.000204\n",
        "score: 0.977700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyjAMw4gs/w8/V+t1NMUyagAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a5b9ad0>"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}