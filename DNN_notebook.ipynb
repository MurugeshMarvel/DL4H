{
 "metadata": {
  "name": "",
  "signature": "sha256:80c8306e8e5d605923c27ca1ace1747ee45d0e7fb7c7f35b45550b53dfb3d8fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\"\n",
      "A deep neural network with or w/o dropout in one notebook.\n",
      "\n",
      "To train the networks you just need to call the run() function\n",
      "You need to run each cell before doing so.\n",
      "To do so, you can go to \"Cell -> Run All\"\n",
      "Feel free to edit individually each cell and rerun modified ones.\n",
      "\n",
      "You can change global parameters here before running\n",
      "\"\"\"\n",
      "\n",
      "# Which experiment to run\n",
      "MNIST = True\n",
      "DIGITS = False\n",
      "FACES = False\n",
      "TWENTYNEWSGROUPS = False\n",
      "\n",
      "SCALE = True      # preprocessing scale \n",
      "BATCH_SIZE = 100  # default batch size\n",
      "L2_LAMBDA = 1.    # default L2 regularization parameter\n",
      "INIT_LR = 0.01    # initial learning rate, try making it larger\n",
      "\n",
      "#run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import sys\n",
      "import math\n",
      "from theano import tensor as T\n",
      "from theano import shared\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from collections import OrderedDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Activation and helper functions\n",
      "\n",
      "def relu_f(vec):\n",
      "    \"\"\" Wrapper to quickly change the rectified linear unit function \"\"\"\n",
      "    return (vec + abs(vec)) / 2.\n",
      "\n",
      "\n",
      "def softplus_f(v):\n",
      "    return T.log(1 + T.exp(v))\n",
      "\n",
      "\n",
      "def dropout(rng, x, p=0.5):\n",
      "    \"\"\" Zero-out random values in x with probability p using rng \"\"\"\n",
      "    if p > 0. and p < 1.:\n",
      "        seed = rng.randint(2 ** 30)\n",
      "        srng = theano.tensor.shared_randomstreams.RandomStreams(seed)\n",
      "        mask = srng.binomial(n=1, p=1.-p, size=x.shape,\n",
      "                dtype=theano.config.floatX)\n",
      "        return x * mask\n",
      "    return x\n",
      "\n",
      "\n",
      "def build_shared_zeros(shape, name):\n",
      "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
      "    return shared(value=numpy.zeros(shape, dtype=theano.config.floatX),\n",
      "            name=name, borrow=True)\n",
      "\n",
      "\n",
      "class Linear(object):\n",
      "    \"\"\" Basic linear transformation layer (W.X + b) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(rng.uniform(\n",
      "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "            W_values *= 4  # This works for sigmoid activated networks!\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "        if b is None:\n",
      "            b = build_shared_zeros((n_out,), 'b')\n",
      "        self.input = input\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "        self.params = [self.W, self.b]\n",
      "        self.output = T.dot(self.input, self.W) + self.b\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"Linear\"\n",
      "\n",
      "\n",
      "class SigmoidLayer(Linear):\n",
      "    \"\"\" Sigmoid activation layer (sigmoid(W.X + b)) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        super(SigmoidLayer, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = T.nnet.sigmoid(self.pre_activation)\n",
      "\n",
      "\n",
      "class ReLU(Linear):\n",
      "    \"\"\" Rectified Linear Unit activation layer (max(0, W.X + b)) \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if b is None:\n",
      "            b = build_shared_zeros((n_out,), 'b')\n",
      "        super(ReLU, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = relu_f(self.pre_activation)\n",
      "\n",
      "\n",
      "class SoftPlus(Linear):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "        super(SoftPlus, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "        self.pre_activation = self.output\n",
      "        self.output = softplus_f(self.pre_activation)\n",
      "\n",
      "\n",
      "class DatasetMiniBatchIterator(object):\n",
      "    \"\"\" Basic mini-batch iterator \"\"\"\n",
      "    def __init__(self, x, y, batch_size=BATCH_SIZE, randomize=False):\n",
      "        self.x = x\n",
      "        self.y = y\n",
      "        self.batch_size = batch_size\n",
      "        self.randomize = randomize\n",
      "        from sklearn.utils import check_random_state\n",
      "        self.rng = check_random_state(42)\n",
      "\n",
      "    def __iter__(self):\n",
      "        n_samples = self.x.shape[0]\n",
      "        if self.randomize:\n",
      "            for _ in xrange(n_samples / BATCH_SIZE):\n",
      "                if BATCH_SIZE > 1:\n",
      "                    i = int(self.rng.rand(1) * ((n_samples+BATCH_SIZE-1) / BATCH_SIZE))\n",
      "                else:\n",
      "                    i = int(math.floor(self.rng.rand(1) * n_samples))\n",
      "                yield (i, self.x[i*self.batch_size:(i+1)*self.batch_size],\n",
      "                       self.y[i*self.batch_size:(i+1)*self.batch_size])\n",
      "        else:\n",
      "            for i in xrange((n_samples + self.batch_size - 1)\n",
      "                            / self.batch_size):\n",
      "                yield (self.x[i*self.batch_size:(i+1)*self.batch_size],\n",
      "                       self.y[i*self.batch_size:(i+1)*self.batch_size])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression:\n",
      "    \"\"\" _Multi-class_ Logistic Regression \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        if W != None:\n",
      "            self.W = W\n",
      "        else:\n",
      "            self.W = build_shared_zeros((n_in, n_out), 'W')\n",
      "        if b != None:\n",
      "            self.b = b\n",
      "        else:\n",
      "            self.b = build_shared_zeros((n_out,), 'b')\n",
      "        self.input = input\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        self.output = self.y_pred\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def negative_log_likelihood_sum(self, y):\n",
      "        return -T.sum(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def training_cost(self, y):\n",
      "        \"\"\" Wrapper for standard name \"\"\"\n",
      "        return self.negative_log_likelihood(y)\n",
      "\n",
      "    def errors(self, y):\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\"!!! 'y' should have the same shape as 'self.y_pred'\",\n",
      "                (\"y\", y.type, \"y_pred\", self.y_pred.type))\n",
      "        if y.dtype.startswith('int'):\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            print(\"!!! y should be of int type\")\n",
      "            return T.mean(T.neq(self.y_pred, numpy.asarray(y, dtype='int')))\n",
      "        \n",
      "        \n",
      "#class PerceptronLoss: # TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HingeLoss(LogisticRegression):\n",
      "    \"\"\" _Multi-class_ Logistic Regression \"\"\"\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
      "        super(HingeLoss, self).__init__(rng, input, n_in, n_out, W, b)\n",
      "\n",
      "    def hinge_loss(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[:,y]) # TODO\n",
      "\n",
      "    def hinge_loss_sum(self, y):\n",
      "        return -T.sum(T.log(self.p_y_given_x)[:,y])\n",
      "\n",
      "    def training_cost(self, y):\n",
      "        \"\"\" Wrapper for standard name \"\"\"\n",
      "        return self.negative_log_likelihood(y)\n",
      "\n",
      "    def errors(self, y):\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\"!!! 'y' should have the same shape as 'self.y_pred'\",\n",
      "                (\"y\", y.type, \"y_pred\", self.y_pred.type))\n",
      "        if y.dtype.startswith('int'):\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            print(\"!!! y should be of int type\")\n",
      "            return T.mean(T.neq(self.y_pred, numpy.asarray(y, dtype='int')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNet(object):\n",
      "    \"\"\" Neural network (not regularized, without dropout) \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None, \n",
      "                 n_ins=40*3,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[1024, 1024, 1024, 1024],\n",
      "                 n_outs=62*3,\n",
      "                 rho=0.95, eps=1.E-6,\n",
      "                 momentum=0.9, step_adapt_alpha=1.E-4,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        Basic Neural Net class\n",
      "        \"\"\"\n",
      "        self.layers = []\n",
      "        self.params = []\n",
      "        self.n_layers = len(layers_types)\n",
      "        self.layers_types = layers_types\n",
      "        assert self.n_layers > 0\n",
      "        self._rho = rho  # ``momentum'' for adadelta (and discount/decay for RMSprop)\n",
      "        self._eps = eps  # epsilon for adadelta (and for RMSprop)\n",
      "        self._momentum = momentum  # for RMSProp\n",
      "        self._accugrads = []  # for adadelta\n",
      "        self._accudeltas = []  # for adadelta\n",
      "        self._avggrads = []  # for RMSprop in the Alex Graves' variant\n",
      "        self._stepadapts = []  # for RMSprop with step adaptations\n",
      "        self._stepadapt_alpha = step_adapt_alpha\n",
      "\n",
      "        if theano_rng == None:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        self.x = T.fmatrix('x')\n",
      "        self.y = T.ivector('y')\n",
      "        \n",
      "        self.layers_ins = [n_ins] + layers_sizes\n",
      "        self.layers_outs = layers_sizes + [n_outs]\n",
      "        \n",
      "        layer_input = self.x\n",
      "        \n",
      "        for layer_type, n_in, n_out in zip(layers_types,\n",
      "                self.layers_ins, self.layers_outs):\n",
      "            this_layer = layer_type(rng=numpy_rng,\n",
      "                    input=layer_input, n_in=n_in, n_out=n_out)\n",
      "            assert hasattr(this_layer, 'output')\n",
      "            self.params.extend(this_layer.params)\n",
      "            self._accugrads.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'accugrad') for t in this_layer.params])\n",
      "            self._accudeltas.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'accudelta') for t in this_layer.params])\n",
      "            self._avggrads.extend([build_shared_zeros(t.shape.eval(),\n",
      "                'avggrad') for t in this_layer.params])\n",
      "            self._stepadapts.extend([shared(value=numpy.ones(t.shape.eval(),\n",
      "                dtype=theano.config.floatX),\n",
      "                name='stepadapt', borrow=True) for t in this_layer.params])\n",
      "            self.layers.append(this_layer)\n",
      "            layer_input = this_layer.output\n",
      "\n",
      "        assert hasattr(self.layers[-1], 'training_cost')\n",
      "        assert hasattr(self.layers[-1], 'errors')\n",
      "        self.mean_cost = self.layers[-1].negative_log_likelihood(self.y)\n",
      "        self.cost = self.layers[-1].training_cost(self.y)\n",
      "        if debugprint:\n",
      "            theano.printing.debugprint(self.cost)\n",
      "\n",
      "        self.errors = self.layers[-1].errors(self.y)\n",
      "\n",
      "    def __repr__(self):\n",
      "        dimensions_layers_str = map(lambda x: \"x\".join(map(str, x)),\n",
      "                                    zip(self.layers_ins, self.layers_outs))\n",
      "        return \"_\".join(map(lambda x: \"_\".join((x[0].__name__, x[1])),\n",
      "                            zip(self.layers_types, dimensions_layers_str)))\n",
      "\n",
      "\n",
      "    def get_SGD_trainer(self):\n",
      "        \"\"\" Returns a plain SGD minibatch trainer with learning rate as param. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)  # all the gradients\n",
      "        updates = OrderedDict()\n",
      "        for param, gparam in zip(self.params, gparams):\n",
      "            updates[param] = param - gparam * learning_rate\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y),\n",
      "                                           theano.Param(learning_rate)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_adagrad_trainer(self):\n",
      "        \"\"\" Returns an Adagrad (Duchi et al. 2010) trainer using a learning rate.\n",
      "        \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)  # all the gradients\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, param, gparam in zip(self._accugrads, self.params, gparams):\n",
      "            # c.f. Algorithm 1 in the Adadelta paper (Zeiler 2012)\n",
      "            agrad = accugrad + gparam * gparam\n",
      "            dx = - (learning_rate / T.sqrt(agrad + self._eps)) * gparam\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = agrad\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x), \n",
      "            theano.Param(batch_y),\n",
      "            theano.Param(learning_rate)],\n",
      "            outputs=self.mean_cost,\n",
      "            updates=updates,\n",
      "            givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_adadelta_trainer(self):\n",
      "        \"\"\" Returns an Adadelta (Zeiler 2012) trainer using self._rho and\n",
      "        self._eps params. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        gparams = T.grad(self.mean_cost, self.params)\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, accudelta, param, gparam in zip(self._accugrads,\n",
      "                self._accudeltas, self.params, gparams):\n",
      "            # c.f. Algorithm 1 in the Adadelta paper (Zeiler 2012)\n",
      "            agrad = self._rho * accugrad + (1 - self._rho) * gparam * gparam\n",
      "            dx = - T.sqrt((accudelta + self._eps)\n",
      "                          / (agrad + self._eps)) * gparam\n",
      "            updates[accudelta] = (self._rho * accudelta\n",
      "                                  + (1 - self._rho) * dx * dx)\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = agrad\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def get_rmsprop_trainer(self, with_step_adapt=True, nesterov=False):  # TODO Nesterov momentum\n",
      "        \"\"\" Returns an RmsProp (possibly Nesterov) (Sutskever 2013) trainer\n",
      "        using self._rho, self._eps and self._momentum params. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        learning_rate = T.fscalar('lr')  # learning rate\n",
      "        gparams = T.grad(self.mean_cost, self.params)\n",
      "        updates = OrderedDict()\n",
      "        for accugrad, avggrad, accudelta, sa, param, gparam in zip(\n",
      "                self._accugrads, self._avggrads, self._accudeltas,\n",
      "                self._stepadapts, self.params, gparams):\n",
      "            acc_grad = self._rho * accugrad + (1 - self._rho) * gparam * gparam\n",
      "            avg_grad = self._rho * avggrad + (1 - self._rho) * gparam  # this decay/discount (self._rho) should differ from the one of the line above\n",
      "            ###scaled_grad = gparam / T.sqrt(acc_grad + self._eps)  # original RMSprop gradient scaling\n",
      "            scaled_grad = gparam / T.sqrt(acc_grad - avg_grad**2 + self._eps)  # Alex Graves' RMSprop variant (divide by a \"running stddev\" of the updates)\n",
      "            if with_step_adapt:\n",
      "                incr = sa * (1. + self._stepadapt_alpha)\n",
      "                #decr = sa * (1. - self._stepadapt_alpha)\n",
      "                decr = sa * (1. - 2*self._stepadapt_alpha)\n",
      "                ###steps = sa * T.switch(accudelta * -gparam >= 0, incr, decr)\n",
      "                steps = T.clip(T.switch(accudelta * -gparam >= 0, incr, decr), self._eps, 1./self._eps)  # bad overloading of self._eps!\n",
      "                scaled_grad = steps * scaled_grad\n",
      "                updates[sa] = steps\n",
      "            dx = self._momentum * accudelta - learning_rate * scaled_grad\n",
      "            updates[param] = param + dx\n",
      "            updates[accugrad] = acc_grad\n",
      "            updates[avggrad] = avg_grad\n",
      "            updates[accudelta] = dx\n",
      "\n",
      "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                           theano.Param(batch_y),\n",
      "                                           theano.Param(learning_rate)],\n",
      "                                   outputs=self.mean_cost,\n",
      "                                   updates=updates,\n",
      "                                   givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        return train_fn\n",
      "\n",
      "    def score_classif(self, given_set):\n",
      "        \"\"\" Returns functions to get current classification errors. \"\"\"\n",
      "        batch_x = T.fmatrix('batch_x')\n",
      "        batch_y = T.ivector('batch_y')\n",
      "        score = theano.function(inputs=[theano.Param(batch_x),\n",
      "                                        theano.Param(batch_y)],\n",
      "                                outputs=self.errors,\n",
      "                                givens={self.x: batch_x, self.y: batch_y})\n",
      "\n",
      "        def scoref():\n",
      "            \"\"\" returned function that scans the entire set given as input \"\"\"\n",
      "            return [score(batch_x, batch_y) for batch_x, batch_y in given_set]\n",
      "\n",
      "        return scoref\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RegularizedNet(NeuralNet):\n",
      "    \"\"\" Neural net with L1 and L2 regularization \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None,\n",
      "                 n_ins=100,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[1024, 1024, 1024],\n",
      "                 n_outs=2,\n",
      "                 rho=0.95, eps=1.E-6,\n",
      "                 L1_reg=0.1,\n",
      "                 L2_reg=0.1,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        A deep neural net with possible L1 and/or L2 regularization.\n",
      "        \"\"\"\n",
      "        super(RegularizedNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
      "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
      "\n",
      "        self.L1_reg = L1_reg\n",
      "        self.L2_reg = L2_reg\n",
      "        L1 = shared(0.)\n",
      "        for param in self.params:\n",
      "            L1 += T.sum(abs(param))\n",
      "        if L1_reg > 0.:\n",
      "            self.cost = self.cost + L1_reg * L1\n",
      "        L2 = shared(0.)\n",
      "        for param in self.params:\n",
      "            L2 += T.sum(param ** 2)\n",
      "        if L2_reg > 0.:\n",
      "            self.cost = self.cost + L2_reg * L2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DropoutNet(NeuralNet):\n",
      "    \"\"\" Neural net with dropout (see Hinton's et al. paper) \"\"\"\n",
      "    def __init__(self, numpy_rng, theano_rng=None,\n",
      "                 n_ins=40*3,\n",
      "                 layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                 layers_sizes=[4000, 4000, 4000, 4000],\n",
      "                 dropout_rates=[0.2, 0.5, 0.5, 0.5, 0.5],\n",
      "                 n_outs=62 * 3,\n",
      "                 rho=0.98, eps=1.E-6,\n",
      "                 debugprint=False):\n",
      "        \"\"\"\n",
      "        A dropout-regularized neural net.\n",
      "        \"\"\"\n",
      "        super(DropoutNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
      "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
      "\n",
      "        self.dropout_rates = dropout_rates\n",
      "        dropout_layer_input = dropout(numpy_rng, self.x, p=dropout_rates[0])\n",
      "        self.dropout_layers = []\n",
      "\n",
      "        for layer, layer_type, n_in, n_out, dr in zip(self.layers,\n",
      "                layers_types, self.layers_ins, self.layers_outs,\n",
      "                dropout_rates[1:] + [0]):  # !!! we do not dropout anything\n",
      "                                           # from the last layer !!!\n",
      "            if dr:\n",
      "                this_layer = layer_type(rng=numpy_rng,\n",
      "                        input=dropout_layer_input, n_in=n_in, n_out=n_out,\n",
      "                        W=layer.W * 1. / (1. - dr),\n",
      "                        b=layer.b * 1. / (1. - dr))\n",
      "                # N.B. dropout with dr==1 does not dropanything!!\n",
      "                this_layer.output = dropout(numpy_rng, this_layer.output, dr)\n",
      "            else:\n",
      "                this_layer = layer_type(rng=numpy_rng,\n",
      "                        input=dropout_layer_input, n_in=n_in, n_out=n_out,\n",
      "                        W=layer.W, b=layer.b)\n",
      "\n",
      "            assert hasattr(this_layer, 'output')\n",
      "            self.dropout_layers.append(this_layer)\n",
      "            dropout_layer_input = this_layer.output\n",
      "\n",
      "        assert hasattr(self.layers[-1], 'training_cost')\n",
      "        assert hasattr(self.layers[-1], 'errors')\n",
      "        # TODO standardize cost\n",
      "        # these are the dropout costs\n",
      "        self.mean_cost = self.dropout_layers[-1].negative_log_likelihood(self.y)\n",
      "        self.cost = self.dropout_layers[-1].training_cost(self.y)\n",
      "\n",
      "        # these is the non-dropout errors\n",
      "        self.errors = self.layers[-1].errors(self.y)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return super(DropoutNet, self).__repr__() + \"\\n\"\\\n",
      "                + \"dropout rates: \" + str(self.dropout_rates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_fit_and_score(class_to_chg):\n",
      "    \"\"\" Mutates a class to add the fit() and score() functions to a NeuralNet.\n",
      "    \"\"\"\n",
      "    from types import MethodType\n",
      "    def fit(self, x_train, y_train, x_dev=None, y_dev=None,\n",
      "            max_epochs=20, early_stopping=True, split_ratio=0.1, # TODO 100+ epochs\n",
      "            method='adadelta', verbose=False, plot=False):\n",
      "        \"\"\"\n",
      "        TODO\n",
      "        \"\"\"\n",
      "        import time, copy\n",
      "        if x_dev == None or y_dev == None:\n",
      "            from sklearn.cross_validation import train_test_split\n",
      "            x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train,\n",
      "                    test_size=split_ratio, random_state=42)\n",
      "        if method == 'sgd':\n",
      "            train_fn = self.get_SGD_trainer()\n",
      "        elif method == 'adagrad':\n",
      "            train_fn = self.get_adagrad_trainer()\n",
      "        elif method == 'adadelta':\n",
      "            train_fn = self.get_adadelta_trainer()\n",
      "        elif method == 'rmsprop':\n",
      "            train_fn = self.get_rmsprop_trainer(with_step_adapt=True,\n",
      "                    nesterov=False)\n",
      "        train_set_iterator = DatasetMiniBatchIterator(x_train, y_train)\n",
      "        dev_set_iterator = DatasetMiniBatchIterator(x_dev, y_dev)\n",
      "        train_scoref = self.score_classif(train_set_iterator)\n",
      "        dev_scoref = self.score_classif(dev_set_iterator)\n",
      "        best_dev_loss = numpy.inf\n",
      "        epoch = 0\n",
      "        # TODO early stopping (not just cross val, also stop training)\n",
      "        if plot:\n",
      "            verbose = True\n",
      "            self._costs = []\n",
      "            self._train_errors = []\n",
      "            self._dev_errors = []\n",
      "            self._updates = []\n",
      "\n",
      "        init_lr = INIT_LR\n",
      "        if method == 'rmsprop':\n",
      "            init_lr = 1.E-6  # TODO REMOVE HACK\n",
      "        n_seen = 0\n",
      "        while epoch < max_epochs:\n",
      "            #lr = init_lr / (1 + init_lr * L2_LAMBDA * math.log(1+n_seen))\n",
      "            #lr = init_lr / math.sqrt(1 + init_lr * L2_LAMBDA * n_seen/BATCH_SIZE) # try these\n",
      "            lr = init_lr\n",
      "            if not verbose:\n",
      "                sys.stdout.write(\"\\r%0.2f%%\" % (epoch * 100./ max_epochs))\n",
      "                sys.stdout.flush()\n",
      "            avg_costs = []\n",
      "            timer = time.time()\n",
      "            for x, y in train_set_iterator:\n",
      "                if method == 'sgd' or method == 'adagrad' or method == 'rmsprop':\n",
      "                    #avg_cost = train_fn(x, y, lr=1.E-2)\n",
      "                    avg_cost = train_fn(x, y, lr=lr)\n",
      "                elif method == 'adadelta':\n",
      "                    avg_cost = train_fn(x, y)\n",
      "                elif method == 'rmsprop':\n",
      "                    avg_cost = train_fn(x, y, lr=lr)\n",
      "                if type(avg_cost) == list:\n",
      "                    avg_costs.append(avg_cost[0])\n",
      "                else:\n",
      "                    avg_costs.append(avg_cost)\n",
      "            if verbose:\n",
      "                mean_costs = numpy.mean(avg_costs)\n",
      "                mean_train_errors = numpy.mean(train_scoref())\n",
      "                print('  epoch %i took %f seconds' %\n",
      "                      (epoch, time.time() - timer))\n",
      "                print('  epoch %i, avg costs %f' %\n",
      "                      (epoch, mean_costs))\n",
      "                print('  epoch %i, training error %f' %\n",
      "                      (epoch, mean_train_errors))\n",
      "                if plot:\n",
      "                    self._costs.append(mean_costs)\n",
      "                    self._train_errors.append(mean_train_errors)\n",
      "            dev_errors = numpy.mean(dev_scoref())\n",
      "            if plot:\n",
      "                self._dev_errors.append(dev_errors)\n",
      "            if dev_errors < best_dev_loss:\n",
      "                best_dev_loss = dev_errors\n",
      "                best_params = copy.deepcopy(self.params)\n",
      "                if verbose:\n",
      "                    print('!!!  epoch %i, validation error of best model %f' %\n",
      "                          (epoch, dev_errors))\n",
      "            epoch += 1\n",
      "            n_seen += x_train.shape[0]\n",
      "        if not verbose:\n",
      "            print(\"\")\n",
      "        for i, param in enumerate(best_params):\n",
      "            self.params[i] = param\n",
      "\n",
      "    def score(self, x, y):\n",
      "        \"\"\" error rates \"\"\"\n",
      "        iterator = DatasetMiniBatchIterator(x, y)\n",
      "        scoref = self.score_classif(iterator)\n",
      "        return numpy.mean(scoref())\n",
      "\n",
      "    class_to_chg.fit = MethodType(fit, None, class_to_chg)\n",
      "    class_to_chg.score = MethodType(score, None, class_to_chg)\n",
      "\n",
      "\n",
      "def train_models(x_train, y_train, x_test, y_test, n_features, n_outs,\n",
      "        x_dev=None, y_dev=None,\n",
      "        use_dropout=False, n_epochs=100, numpy_rng=None,\n",
      "        svms=False, nb=False, deepnn=True,\n",
      "        verbose=False, plot=False, name=''):\n",
      "    if svms:\n",
      "        print(\"Linear SVM\")\n",
      "        classifier = svm.SVC(gamma=0.001)\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "        print(\"RBF-kernel SVM\")\n",
      "        classifier = svm.SVC(kernel='rbf', class_weight='auto')\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "    if nb:\n",
      "        print(\"Multinomial Naive Bayes\")\n",
      "        classifier = naive_bayes.MultinomialNB()\n",
      "        print(classifier)\n",
      "        classifier.fit(x_train, y_train)\n",
      "        print(\"score: %f\" % classifier.score(x_test, y_test))\n",
      "\n",
      "    if deepnn:\n",
      "        import warnings\n",
      "        warnings.filterwarnings(\"ignore\")  # TODO remove\n",
      "\n",
      "        if use_dropout:\n",
      "            n_epochs *= 4\n",
      "            pass\n",
      "\n",
      "        def new_dnn(dropout=False):\n",
      "            if dropout:\n",
      "                print(\"Dropout DNN\")\n",
      "                return DropoutNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
      "                    #layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                    #layers_sizes=[1000, 1000, 1000],\n",
      "                    #dropout_rates=[0., 0.5, 0.5, 0.5],\n",
      "                    layers_types=[ReLU, ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                    layers_sizes=[2000, 2000, 2000, 2000],\n",
      "                    dropout_rates=[0.2, 0.5, 0.5, 0.5, 0.5],\n",
      "                    n_outs=n_outs,\n",
      "                    debugprint=0)\n",
      "            else:\n",
      "                print(\"Simple (regularized) DNN\")\n",
      "                return RegularizedNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
      "                    #layers_types=[LogisticRegression],\n",
      "                    #layers_sizes=[],\n",
      "                    layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
      "                    layers_sizes=[1000, 1000, 1000],\n",
      "                    #layers_types=[ReLU, LogisticRegression],\n",
      "                    #layers_sizes=[200],\n",
      "                    n_outs=n_outs,\n",
      "                    L1_reg=0.,\n",
      "                    L2_reg=L2_LAMBDA,\n",
      "                    debugprint=1)\n",
      "\n",
      "        import matplotlib.pyplot as plt\n",
      "        plt.figure()\n",
      "        ax1 = plt.subplot(221)\n",
      "        ax2 = plt.subplot(222)\n",
      "        ax3 = plt.subplot(223)\n",
      "        ax4 = plt.subplot(224)  # TODO updates of the weights\n",
      "        #methods = ['sgd', 'adagrad', 'adadelta']\n",
      "        #methods = ['adagrad', 'adadelta']\n",
      "        methods = ['rmsprop', 'adadelta']\n",
      "        #methods = ['rmsprop', 'adadelta', 'adagrad']\n",
      "        for method in methods:\n",
      "            dnn = new_dnn(use_dropout)\n",
      "            print dnn\n",
      "            dnn.fit(x_train, y_train, x_dev, y_dev, max_epochs=n_epochs,\n",
      "                    method=method, verbose=verbose, plot=plot)\n",
      "            test_error = dnn.score(x_test, y_test)\n",
      "            print(\"score: %f\" % (1. - test_error))\n",
      "            ax1.plot(numpy.log10(dnn._costs), label=method)\n",
      "            #ax2.plot(numpy.log10(dnn._train_errors), label=method)\n",
      "            #ax3.plot(numpy.log10(dnn._dev_errors), label=method)\n",
      "            ax2.plot(dnn._train_errors, label=method)\n",
      "            ax3.plot(dnn._dev_errors, label=method)\n",
      "            #ax4.plot(dnn._updates, label=method) TODO\n",
      "            ax4.plot([test_error for _ in range(10)], label=method)\n",
      "        ax1.set_xlabel('epoch')\n",
      "        ax1.set_ylabel('cost (log10)')\n",
      "        ax2.set_xlabel('epoch')\n",
      "        ax2.set_ylabel('train error')\n",
      "        ax3.set_xlabel('epoch')\n",
      "        ax3.set_ylabel('dev error')\n",
      "        ax4.set_ylabel('test error')\n",
      "        plt.legend()\n",
      "        plt.tight_layout()\n",
      "        plt.savefig('training_' + name + '.png')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#main function\n",
      "\n",
      "def run():\n",
      "    add_fit_and_score(DropoutNet)\n",
      "    add_fit_and_score(RegularizedNet)\n",
      "\n",
      "    def nudge_dataset(X, Y):\n",
      "        \"\"\"\n",
      "        This produces a dataset 5 times bigger than the original one,\n",
      "        by moving the 8x8 images in X around by 1px to left, right, down, up\n",
      "        \"\"\"\n",
      "        from scipy.ndimage import convolve\n",
      "        direction_vectors = [\n",
      "            [[0, 1, 0],\n",
      "             [0, 0, 0],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [1, 0, 0],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [0, 0, 1],\n",
      "             [0, 0, 0]],\n",
      "            [[0, 0, 0],\n",
      "             [0, 0, 0],\n",
      "             [0, 1, 0]]]\n",
      "        shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
      "                                      weights=w).ravel()\n",
      "        X = numpy.concatenate([X] +\n",
      "                              [numpy.apply_along_axis(shift, 1, X, vector)\n",
      "                                  for vector in direction_vectors])\n",
      "        Y = numpy.concatenate([Y for _ in range(5)], axis=0)\n",
      "        return X, Y\n",
      "\n",
      "    from sklearn import datasets, svm, naive_bayes\n",
      "    from sklearn import cross_validation, preprocessing\n",
      "    \n",
      "\n",
      "    if MNIST:\n",
      "        from sklearn.datasets import fetch_mldata\n",
      "        mnist = fetch_mldata('MNIST original')\n",
      "        X = numpy.asarray(mnist.data, dtype='float32')\n",
      "        if SCALE:\n",
      "            #X = preprocessing.scale(X)\n",
      "            X /= 255.\n",
      "        y = numpy.asarray(mnist.target, dtype='int32')\n",
      "        print(\"Total dataset size:\")\n",
      "        print(\"n samples: %d\" % X.shape[0])\n",
      "        print(\"n features: %d\" % X.shape[1])\n",
      "        print(\"n classes: %d\" % len(set(y)))\n",
      "        x_train, x_test = X[:-10000], X[-10000:]\n",
      "        y_train, y_test = y[:-10000], y[-10000:]\n",
      "\n",
      "        train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "                     len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     use_dropout=False, n_epochs=20,\n",
      "                     verbose=True, plot=True, name='mnist_L2')\n",
      "        #train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "        #             len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "        #             use_dropout=True,\n",
      "        #             verbose=True, plot=True, name='mnist_dropout')\n",
      "\n",
      "    if DIGITS:\n",
      "        digits = datasets.load_digits()\n",
      "        data = numpy.asarray(digits.data, dtype='float32')\n",
      "        target = numpy.asarray(digits.target, dtype='int32')\n",
      "        nudged_x, nudged_y = nudge_dataset(data, target)\n",
      "        if SCALE:\n",
      "            nudged_x = preprocessing.scale(nudged_x)\n",
      "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
      "                nudged_x, nudged_y, test_size=0.2, random_state=42)\n",
      "        train_models(x_train, y_train, x_test, y_test, nudged_x.shape[1],\n",
      "                     len(set(target)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     verbose=True, plot=True, name='digits')\n",
      "\n",
      "    if FACES:\n",
      "        import logging\n",
      "        logging.basicConfig(level=logging.INFO,\n",
      "                            format='%(asctime)s %(message)s')\n",
      "        lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70,\n",
      "                                               resize=0.4)\n",
      "        X = numpy.asarray(lfw_people.data, dtype='float32')\n",
      "        if SCALE:\n",
      "            X = preprocessing.scale(X)\n",
      "        y = numpy.asarray(lfw_people.target, dtype='int32')\n",
      "        target_names = lfw_people.target_names\n",
      "        print(\"Total dataset size:\")\n",
      "        print(\"n samples: %d\" % X.shape[0])\n",
      "        print(\"n features: %d\" % X.shape[1])\n",
      "        print(\"n classes: %d\" % target_names.shape[0])\n",
      "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
      "                    X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "        train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
      "                     len(set(y)), numpy_rng=numpy.random.RandomState(123),\n",
      "                     verbose=True, plot=True, name='faces')\n",
      "\n",
      "    if TWENTYNEWSGROUPS:\n",
      "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "        newsgroups_train = datasets.fetch_20newsgroups(subset='train')\n",
      "        vectorizer = TfidfVectorizer(encoding='latin-1', max_features=10000)\n",
      "        #vectorizer = HashingVectorizer(encoding='latin-1')\n",
      "        x_train = vectorizer.fit_transform(newsgroups_train.data)\n",
      "        x_train = numpy.asarray(x_train.todense(), dtype='float32')\n",
      "        y_train = numpy.asarray(newsgroups_train.target, dtype='int32')\n",
      "        newsgroups_test = datasets.fetch_20newsgroups(subset='test')\n",
      "        x_test = vectorizer.transform(newsgroups_test.data)\n",
      "        x_test = numpy.asarray(x_test.todense(), dtype='float32')\n",
      "        y_test = numpy.asarray(newsgroups_test.target, dtype='int32')\n",
      "        train_models(x_train, y_train, x_test, y_test, x_train.shape[1],\n",
      "                     len(set(y_train)),\n",
      "                     numpy_rng=numpy.random.RandomState(123),\n",
      "                     svms=False, nb=True, deepnn=True,\n",
      "                     verbose=True, plot=True, name='20newsgroups')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}